\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{float}
\usepackage{subcaption}
\usepackage[margin=1in]{geometry}

% Code listing settings
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray}
}

% Title information
\title{\textbf{Object Detection Using Deep Learning and Transfer Learning}\\
       \large Laboratory Report - LAB6}
\author{Audigier Matteo}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report presents a comprehensive investigation of Faster R-CNN for object detection using transfer learning on a custom 94-image dataset. Experiments conducted on an NVIDIA A40 GPU explored hyperparameter optimization, transfer learning strategies, data augmentation, and inference methods. Extended training achieves best results (loss: 0.0763 at 15 epochs), while aggressive learning rates impair convergence (0.252 vs 0.091 baseline). Full fine-tuning outperforms frozen backbone by 139\%, with gradual unfreezing offering a 40\% improvement middle ground. ImageNet normalization proves essential (0.0849 loss), while horizontal flip augmentation fails catastrophically (0.4468 loss). Inference experiments demonstrate good model calibration with well-separated detections requiring no NMS suppression.
\footnote{The source code is vailable at \url{https://github.com/audigiem/AVPR_labs}}
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

\subsection{Context and Objectives}

Object detection is a fundamental computer vision task that involves identifying and localizing objects within images. Unlike image classification, which assigns a single label to an entire image, object detection must predict both the class and bounding box coordinates for multiple objects. This laboratory work implements a state-of-the-art object detection system using Faster R-CNN with transfer learning.

This laboratory work aims to explore the impact of hyperparameters such as learning rate, batch size, and training epochs on training dynamics. We investigate various transfer learning strategies including backbone freezing and gradual unfreezing approaches. Additionally, we evaluate data augmentation techniques for improved model robustness and analyze inference quality using confidence thresholds and Non-Maximum Suppression (NMS).

\subsection{Faster R-CNN Architecture}

Faster R-CNN (Region-based Convolutional Neural Network) is a two-stage object detection framework. The architecture consists of three main components: a \textbf{backbone network} (typically a CNN such as ResNet-50) that extracts feature maps from input images, a \textbf{Region Proposal Network (RPN)} that generates object proposals with objectness scores, and an \textbf{ROI head} that classifies proposals and refines bounding boxes. The architecture particularly benefits from transfer learning, where the backbone is pretrained on ImageNet and subsequently fine-tuned for the specific detection task at hand.

\subsection{Dataset Description}

The custom dataset contains 94 training images with YOLO-format annotations specifying class labels and normalized bounding box coordinates (x\_center, y\_center, width, height). The dataset includes 2 object classes (including background). The relatively small dataset size makes transfer learning essential and increases the risk of overfitting, necessitating careful hyperparameter selection and validation strategies.

\subsection{Computational Environment}

All experiments were conducted on the university cluster with the following specifications:
\begin{itemize}
    \item \textbf{GPU}: NVIDIA A40 (46GB VRAM)
    \item \textbf{CUDA Version}: 12.8
    \item \textbf{PyTorch Version}: 2.9.1+cu128
    \item \textbf{cuDNN Version}: 91002
    \item \textbf{Total Training Time}: $\sim$2 hours for all tasks
\end{itemize}

The A40's substantial computational power enabled extensive hyperparameter exploration with increased epoch counts compared to standard configurations.

\section{Methodology}

\subsection{Faster R-CNN Implementation}

The implementation uses PyTorch's \texttt{torchvision.models.detection} module, which provides pretrained Faster R-CNN models. Key components:

\begin{lstlisting}
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

# Load pretrained model
model = fasterrcnn_resnet50_fpn(pretrained=True)

# Modify classification head for custom classes
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
\end{lstlisting}

\subsection{Custom Dataset Implementation}

A custom PyTorch Dataset class handles YOLO-format annotations and converts them to the format expected by Faster R-CNN:

\begin{lstlisting}
class CustomDataset(Dataset):
    def __getitem__(self, idx):
        # Load image
        image = Image.open(image_path).convert("RGB")
        
        # Parse YOLO annotations and convert to [x1, y1, x2, y2]
        boxes = convert_yolo_to_xyxy(yolo_annotations, img_width, img_height)
        
        # Create target dictionary
        target = {
            "boxes": torch.as_tensor(boxes, dtype=torch.float32),
            "labels": torch.as_tensor(labels, dtype=torch.int64)
        }
        
        return transform(image), target
\end{lstlisting}

\subsection{Training Procedure}

The training loop follows standard supervised learning:

\begin{enumerate}
    \item \textbf{Forward pass}: Model computes losses internally (classification + bbox regression)
    \item \textbf{Backward pass}: Compute gradients via backpropagation
    \item \textbf{Optimization}: Update parameters using SGD optimizer
    \item \textbf{Loss tracking}: Monitor average epoch loss for convergence
\end{enumerate}

The model outputs multiple loss components that are automatically summed:
\begin{itemize}
    \item RPN classification loss
    \item RPN bounding box regression loss
    \item ROI classification loss
    \item ROI bounding box regression loss
\end{itemize}

\subsection{Transfer Learning Strategies}

Three transfer learning approaches were implemented:

\begin{enumerate}
    \item \textbf{No Freezing}: Train all parameters (full fine-tuning)
    \item \textbf{Frozen Backbone}: Freeze all backbone layers, train only ROI heads
    \item \textbf{Gradual Unfreezing}: Freeze backbone but unfreeze the last block (layer4)
\end{enumerate}

Freezing is implemented by setting \texttt{requires\_grad=False} for specific parameters:

\begin{lstlisting}
# Freeze backbone
for param in model.backbone.parameters():
    param.requires_grad = False

# Optionally unfreeze layer4
if unfreeze_layer4:
    for param in model.backbone.body.layer4.parameters():
        param.requires_grad = True
\end{lstlisting}

\section{Task 1: Hyperparameter Exploration}

\subsection{Experimental Setup}

Six configurations were tested to understand the impact of learning rate, batch size, and training epochs. The increased computational power of the A40 GPU allowed for more extensive exploration than typically feasible.

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Configuration} & \textbf{Learning Rate} & \textbf{Batch Size} & \textbf{Epochs} & \textbf{Total Iterations} \\
\midrule
config1\_baseline & 0.0001 & 1 & 10 & 940 \\
config2\_high\_lr & 0.001 & 1 & 10 & 940 \\
config3\_low\_lr & 0.00001 & 1 & 10 & 940 \\
config4\_batch2 & 0.0001 & 2 & 8 & 376 \\
config5\_batch4 & 0.0001 & 4 & 8 & 188 \\
config6\_more\_epochs & 0.0001 & 1 & 15 & 1,410 \\
\bottomrule
\end{tabular}
\caption{Hyperparameter configurations tested in Task 1}
\label{tab:task1_configs}
\end{table}

\subsection{Results}

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Configuration} & \textbf{Final Loss} & \textbf{Training Time (s)} & \textbf{Convergence} \\
\midrule
config6\_more\_epochs & \textbf{0.0763} & 470.90 & \textbf{Excellent} \\
config5\_batch4 & 0.0972 & 254.65 & Good \\
config1\_baseline & 0.0906 & 380.74 & Good \\
config3\_low\_lr & 0.0927 & 313.74 & Good \\
config4\_batch2 & 0.1237 & 257.61 & Moderate \\
config2\_high\_lr & 0.2518 & 314.92 & Poor \\
\bottomrule
\end{tabular}
\caption{Task 1 results: Hyperparameter exploration performance}
\label{tab:task1_results}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{outputs/plots/task1_hyperparameter_exploration.png}
\caption{Training curves for all hyperparameter configurations showing convergence patterns, with extended training (15 epochs) achieving best performance.}
\label{fig:task1_curves}
\end{figure}

% \begin{figure}[H]
% \centering
% \includegraphics[width=0.85\textwidth]{outputs/plots/task1_final_comparison.png}
% \caption{Final loss and training time comparison for Task 1 configurations. Config6 achieves the best final loss with moderate training time.}
% \label{fig:task1_comparison}
% \end{figure}

\subsection{Analysis}

\subsubsection{Learning Rate Impact}

The learning rate profoundly affects training stability and convergence:

\textbf{Baseline (LR=0.0001):} Achieves stable convergence with final loss of 0.0906, demonstrating consistent improvement across all 10 epochs. The loss progression is smooth: $0.335 \rightarrow 0.239 \rightarrow 0.216 \rightarrow ... \rightarrow 0.091$.

\textbf{High Learning Rate (LR=0.001):} Shows unstable training with poor final convergence. Initial epochs display rapid but erratic loss reduction ($1.431 \rightarrow 0.760 \rightarrow 0.711$), eventually settling at 0.252, nearly 3x worse than baseline. The aggressive learning rate causes oscillations and prevents fine convergence, demonstrating that pretrained models require conservative learning rates to avoid disrupting carefully learned ImageNet features.

\textbf{Low Learning Rate (LR=0.00001):} Provides the most stable training with gradual, monotonic loss decrease. Final loss of 0.0927 is competitive with baseline. However, the learning is slower initially ($0.468$ at epoch 1 vs $0.335$ for baseline), suggesting that while safe, this rate may require more epochs to reach optimal performance.

\subsubsection{Batch Size Effects}

Batch size influences both computational efficiency and gradient estimation quality:

\textbf{Batch Size 1:} Provides noisy but frequent gradient updates. The baseline configuration achieves loss of 0.0906 with 940 total iterations. The high variance in gradients can help escape local minima but may slow convergence.

\textbf{Batch Size 2:} Reduces total iterations to 376 while maintaining reasonable performance (loss=0.1237). Training time decreases to 257.61s (32\% reduction). However, the final loss is 36\% higher than baseline, suggesting that gradient quality suffers with fewer, slightly smoothed updates.

\textbf{Batch Size 4:} Achieves the best time efficiency (254.65s, 33\% reduction) with only 188 iterations. Surprisingly, it outperforms batch size 2 with a final loss of 0.0972, approaching baseline performance. This suggests that for this dataset size, moderate batch sizes provide a good balance between gradient quality and computational efficiency.

\subsubsection{Training Duration}

Extended training demonstrates diminishing returns:

\textbf{10 Epochs (Baseline):} Loss decreases from 0.335 to 0.091, showing consistent improvement throughout.

\textbf{15 Epochs (Extended):} Achieves the best final loss (0.0763), representing a 16\% improvement over 10 epochs. The additional 5 epochs show continued learning: epochs 11-15 reduce loss from 0.093 to 0.076. However, training time increases by 24\% (470.90s vs 380.74s), indicating diminishing marginal returns.

The loss progression in the extended configuration shows no signs of overfitting, with losses at epochs 13, 14, and 15 being 0.072, 0.063, and 0.060 respectively. This suggests that even longer training could potentially yield further improvements, though with increasing computational cost.

\subsection{Key Findings}

\begin{enumerate}
    \item \textbf{Learning Rate Sensitivity}: The 10x increase in learning rate (0.0001 $\rightarrow$ 0.001) causes complete training failure, while 10x decrease (0.0001 $\rightarrow$ 0.00001) maintains stability with minimal performance penalty.
    
    \item \textbf{Batch Size Trade-off}: Larger batch sizes provide significant speedup (up to 33\%) with acceptable performance degradation. Batch size 4 offers the best efficiency-performance balance.
    
    \item \textbf{Extended Training Benefits}: The 50\% increase in epochs (10 $\rightarrow$ 15) yields 16\% loss reduction without overfitting, demonstrating that the model still has learning capacity.
    
    \item \textbf{Optimal Configuration}: config6\_more\_epochs (LR=0.0001, BS=1, 15 epochs) achieves the best performance, though config5\_batch4 offers a compelling alternative for time-constrained scenarios.
\end{enumerate}

\section{Task 2: Architectural Adaptation and Transfer Learning}

\subsection{Experimental Setup}

Four transfer learning strategies were evaluated, comparing different backbone freezing configurations and architectures:

\begin{table}[H]
\centering
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Configuration} & \textbf{Backbone} & \textbf{Freeze} & \textbf{Unfreeze L4} & \textbf{Trainable Params} & \textbf{Epochs} \\
\midrule
no\_freeze & ResNet-50 & No & -- & 41,076,761 & 15 \\
freeze\_backbone & ResNet-50 & Yes & No & 14,499,865 & 15 \\
gradual\_unfreeze & ResNet-50 & Yes & Yes & 21,857,817 & 15 \\
mobilenet & MobileNetV3 & No & -- & 13,407,049 & 15 \\
\bottomrule
\end{tabular}
\caption{Transfer learning configurations tested in Task 2}
\label{tab:task2_configs}
\end{table}

Note: Trainable parameters exclude frozen layers. The no\_freeze configuration trains all 41M parameters, while freeze\_backbone trains only the ROI heads (14.5M parameters, 35\% of total).

\subsection{Results}

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Configuration} & \textbf{Final Loss} & \textbf{Training Time (s)} & \textbf{Convergence Quality} \\
\midrule
no\_freeze & \textbf{0.0601} & 474.40 & Excellent \\
gradual\_unfreeze & 0.0862 & 451.29 & Good \\
freeze\_backbone & 0.1435 & 441.85 & Moderate \\
mobilenet & 0.5308 & 434.04 & Poor \\
\bottomrule
\end{tabular}
\caption{Task 2 results: Transfer learning strategy performance}
\label{tab:task2_results}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{outputs/plots/task2_transfer_learning.png}
\caption{Training curves comparing different transfer learning strategies. Full fine-tuning (no\_freeze) achieves significantly lower loss than frozen configurations.}
\label{fig:task2_curves}
\end{figure}

% \begin{figure}[H]
% \centering
% \includegraphics[width=0.85\textwidth]{outputs/plots/task2_comparison.png}
% \caption{Comparison of transfer learning approaches showing final loss, training time, and parameter efficiency.}
% \label{fig:task2_comparison}
% \end{figure}

\subsection{Analysis}

\subsubsection{Full Fine-tuning vs. Frozen Backbone}

\textbf{No Freezing (Full Fine-tuning):} Achieves the best performance with final loss of 0.0601, training all 41M parameters. The training curve shows rapid initial descent ($0.356 \rightarrow 0.218 \rightarrow 0.196$) followed by steady improvement to epoch 15. The model can adapt both low-level features (early backbone layers) and high-level features (late backbone layers + ROI heads) to the specific characteristics of the custom dataset.

\textbf{Frozen Backbone:} Performance degrades significantly (final loss: 0.1435, 139\% higher than no\_freeze) despite training only 35\% of parameters. The training curve shows slower convergence ($0.453 \rightarrow 0.287 \rightarrow 0.252$) and higher final loss. This indicates that the pretrained ImageNet features, while helpful, are not perfectly aligned with the custom dataset's requirements. The ROI heads alone cannot fully compensate for suboptimal backbone features.

The performance gap demonstrates that for this dataset, domain adaptation is crucial. The pretrained features may capture general edges, textures, and shapes from ImageNet, but the specific objects in the custom dataset require feature refinement.

\subsubsection{Gradual Unfreezing Strategy}

The gradual unfreezing approach (freeze backbone but unfreeze layer4) aims to balance feature retention and adaptation:

\textbf{Results:} Achieves intermediate performance (loss: 0.0862) between full fine-tuning and complete freezing. Training 29.4M parameters (71\% of total), it outperforms the frozen configuration by 40\% but still falls short of full fine-tuning by 44\%.

\textbf{Interpretation:} Layer4 (the deepest backbone block) contains the most task-specific features, which benefit from adaptation. However, earlier layers (layer1-3) also encode important information that remains fixed. The results suggest that a fully progressive unfreezing schedule (starting with frozen backbone, then unfreezing layer4, layer3, etc.) might yield better results.

\textbf{Parameter Efficiency:} This configuration offers a middle ground for scenarios with limited computational resources or small datasets where full fine-tuning might cause overfitting.

\subsubsection{Backbone Architecture Comparison}

\textbf{ResNet-50 vs. MobileNetV3:}

MobileNetV3 achieves notably worse performance (loss: 0.5308, 784\% higher than ResNet-50 full fine-tuning) despite faster training (434.04s, 8\% reduction). Several factors explain this gap:

\begin{itemize}
    \item \textbf{Capacity}: ResNet-50 has 41M parameters vs. MobileNetV3's 13M (68\% fewer). The reduced capacity limits the model's ability to learn complex object representations.
    
    \item \textbf{Architecture Design}: MobileNetV3 uses depthwise separable convolutions optimized for mobile deployment, sacrificing some accuracy for efficiency. ResNet-50's standard convolutions provide richer feature representations.
    
    \item \textbf{Pretrained Features}: ImageNet-pretrained ResNet-50 may provide better transferable features than MobileNetV3 for this particular dataset.
    
    \item \textbf{FPN Compatibility}: The Feature Pyramid Network (FPN) component may interact differently with the two backbones, potentially favoring ResNet-50's architecture.
\end{itemize}

\textbf{Use Cases:} While MobileNetV3 underperforms here, it remains valuable for:
\begin{itemize}
    \item Real-time inference on resource-constrained devices
    \item Scenarios where 16\% faster training is critical
    \item Applications where 0.2152 loss is acceptable (e.g., rough localization tasks)
\end{itemize}

\subsection{Trainable Parameters Analysis}

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Configuration} & \textbf{Trainable} & \textbf{Total} & \textbf{\% Trainable} & \textbf{Performance} \\
\midrule
no\_freeze & 41,076,761 & 41,299,161 & 99.5\% & Best \\
gradual\_unfreeze & 29,442,073 & 41,299,161 & 71.3\% & Good \\
freeze\_backbone & 14,499,865 & 41,299,161 & 35.1\% & Moderate \\
mobilenet & 18,871,333 & 18,930,229 & 99.7\% & Poor \\
\bottomrule
\end{tabular}
\caption{Parameter efficiency analysis across configurations}
\label{tab:task2_params}
\end{table}

The analysis reveals a clear correlation: more trainable parameters (when capacity is sufficient) lead to better performance. However, raw parameter count isn't everything—MobileNetV3 trains 98.4\% of its parameters but underperforms frozen ResNet-50 with only 35.1\% trainable parameters, highlighting the importance of architecture design and pretrained feature quality.

\subsection{Key Findings}

Full fine-tuning demonstrates clear superiority, achieving 139\% improvement over frozen backbone and proving that dataset-specific feature adaptation is essential. Gradual unfreezing with layer4 trainable provides 40\% improvement over complete freezing while using 71\% of parameters, offering a balanced middle ground. Architecture selection proves critical, as ResNet-50 outperforms MobileNetV3 by 784\% despite similar training times. For this dataset, the quality advantage of full fine-tuning justifies the 7\% increase in training time compared to frozen backbone. We recommend full fine-tuning with ResNet-50 for optimal results, reserving gradual unfreezing for severely constrained compute scenarios.

\section{Task 3: Data Transformation and Augmentation}

\subsection{Experimental Setup}

Five augmentation strategies were evaluated to assess their impact on model robustness and generalization:

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Configuration} & \textbf{Augmentation Details} \\
\midrule
basic\_transform & ToTensor only (no augmentation) \\
horizontal\_flip & RandomHorizontalFlip(p=0.5) \\
color\_jitter & ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1) \\
normalized & ImageNet Normalization (mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]) \\
combined\_augmentation & Horizontal Flip + Color Jitter + Normalization \\
\bottomrule
\end{tabular}
\caption{Data augmentation configurations tested in Task 3}
\label{tab:task3_configs}
\end{table}

All configurations used: LR=0.0001, Batch Size=1, Epochs=12

\subsection{Results}

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Configuration} & \textbf{Final Loss} & \textbf{Training Time (s)} & \textbf{Stability} \\
\midrule
normalized & \textbf{0.0849} & 434.82 & \textbf{Excellent} \\
color\_jitter & 0.1028 & 1626.66 & Good \\
basic\_transform & 0.1065 & 378.72 & Good \\
horizontal\_flip & \textcolor{red}{0.4468} & 387.85 & \textbf{Poor} \\
combined\_augmentation & \textcolor{red}{0.4479} & 645.84 & \textbf{Poor} \\
\bottomrule
\end{tabular}
\caption{Task 3 results: Data augmentation impact on training}
\label{tab:task3_results}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{outputs/plots/task3_data_augmentation.png}
\caption{Training curves comparing augmentation strategies. ImageNet normalization achieves best performance (0.0849), while horizontal flip and combined augmentation show severe instability.}
\label{fig:task3_curves}
\end{figure}

\subsection{Analysis}

\subsubsection{ImageNet Normalization: Best Performance}

Surprisingly, ImageNet normalization achieves the best performance with final loss of 0.0849, outperforming even the basic transform. Despite showing an initially high loss (0.721), training rapidly converges through $0.721 \rightarrow 0.281 \rightarrow 0.265$, eventually reaching 0.085 at epoch 12. This superior performance stems from aligning the input distribution with what the pretrained ResNet-50 backbone expects, as the backbone's BatchNorm layers were trained on normalized ImageNet data. The normalization also centers input values around zero, providing more stable gradients during backpropagation and enabling better feature extraction from the pretrained layers.

\subsubsection{Basic Transform (Baseline)}

The basic transformation (ToTensor only) achieves competitive performance with final loss of 0.1065 (25.4\% higher than normalized). The training curve shows smooth progression: $0.385 \rightarrow 0.242 \rightarrow 0.184$, eventually reaching 0.107 at epoch 12. This serves as a reasonable baseline but is suboptimal for transfer learning scenarios.

\subsubsection{Color Jitter Augmentation}

ColorJitter achieves reasonable performance (loss: 0.1028, only 3.4\% worse than baseline) with stable training, though requiring significantly longer time (1626s). The augmentation varies brightness, contrast, saturation, and hue within modest ranges:

\begin{lstlisting}
transforms.ColorJitter(
    brightness=0.2,  # +/- 20%
    contrast=0.2,
    saturation=0.2,
    hue=0.1  # +/- 10%
)
\end{lstlisting}

\textbf{Training Dynamics:} Initial loss (0.360) is similar to baseline, but convergence is slower ($0.360 \rightarrow 0.267 \rightarrow 0.237$ vs. $0.385 \rightarrow 0.242 \rightarrow 0.184$ for baseline). Final loss plateaus around 0.138.

\textbf{Interpretation:} The photometric variations force the model to learn color-invariant features, which is beneficial for real-world robustness where lighting conditions vary. However, the performance gap suggests that for this specific dataset, color consistency is actually informative for detection, and removing it harms performance.

\textbf{Use Case:} ColorJitter would be valuable if deploying to environments with different lighting (indoor/outdoor, day/night) than the training set, though the 4x training time increase must be considered.

\subsubsection{Horizontal Flip: A Failure Case}

Horizontal flip augmentation causes severe training instability, achieving the worst performance (loss: 0.4468, 320\% higher than baseline). The training curve reveals the problem:

\textbf{Epoch-by-epoch losses:}
\begin{verbatim}
Epoch  1: 0.756
Epoch  2: 0.512
Epoch  3: 0.510
Epoch  4: 0.515
Epoch  5: 0.500
Epoch  6: 0.515
Epoch  7: 0.452
Epoch  8: 0.414
Epoch  9: 0.422
Epoch 10: 0.381
Epoch 11: 0.415
Epoch 12: 0.447
\end{verbatim}

The loss oscillates dramatically rather than converging, indicating severe training instability. This failure likely stems from multiple compounding factors: objects with directional features (text, arrows, asymmetric shapes) create semantically different examples when flipped, confusing the model. Additionally, with batch size 1 and random flipping (p=0.5), the model alternates between flipped and non-flipped examples, potentially causing gradient conflicts. The bounding box transformation for flipped images may also contain implementation bugs providing inconsistent supervision. Furthermore, the base learning rate (0.0001) may be too high for the increased data diversity from flipping, causing oscillation rather than convergence. Future debugging should focus on visualizing augmented images to verify bounding box correctness, testing deterministic flipping, reducing the learning rate specifically for this augmentation, and examining which object classes are particularly problematic.

\subsubsection{Combined Augmentation: Compounding Instability}

Combining horizontal flip, color jitter, and normalization produces the worst results (loss: 0.4479, 427\% higher than normalized alone). The training curve shows severe instability throughout, with the pattern $0.728 \rightarrow 0.544 \rightarrow 0.524$, then oscillating between 0.429 and 0.523 in the final epochs. Training time increases dramatically to 645.84s (49\% longer than basic transform). The combination of augmentations creates excessive data diversity where multiple augmentations compound the instability from horizontal flipping. Moreover, color jitter and normalization may conflict, with jitter adding variation that normalization tries to suppress, while batch size 1 exacerbates the problem as each update sees very different data characteristics. This demonstrates that \textbf{more augmentation is not always better}, and careful selection and testing of individual augmentations is crucial before combination.

\subsection{Training Time Analysis}

Augmentation computational overhead varies dramatically. Horizontal flip adds minimal overhead at 2.4\% (387.85s vs 378.72s baseline). Color jitter, however, increases training time by 329\% (1626.66s), likely due to per-image processing overhead. Normalization adds 14.8\% (434.82s), while combined augmentation shows 70.5\% overhead (645.84s) from training instability rather than transformation cost. Despite GPU acceleration, color jitter's extreme overhead makes it impractical for routine use.

\subsection{Key Findings}

ImageNet normalization proves essential, achieving 20\% better loss (0.0849 vs 0.1065) than basic transforms by aligning input distribution with pretrained expectations. ColorJitter performs well (0.1028) but with prohibitive 329\% training time overhead. Horizontal flip causes catastrophic instability (0.4468 loss), while combining augmentations compounds failures (0.4479 loss). More augmentation is decidedly not better. For deployment, use ImageNet normalization as standard practice. Consider ColorJitter only for critical lighting robustness needs and when training time permits. Avoid flip-based augmentations entirely until implementation issues are resolved.

\section{Task 4: Evaluation and Inference}

\subsection{Experimental Setup}

Task 4 evaluated inference quality by testing multiple confidence thresholds and NMS configurations on a trained model. The experiment:

\begin{enumerate}
    \item Trained a fresh model on the full dataset (94 images)
    \item Ran inference on a test image: \texttt{DJI\_20250617134732\_0718\_V.JPG}
    \item Tested 4 confidence thresholds: 0.3, 0.5, 0.7, 0.9
    \item Tested 3 NMS IoU thresholds: 0.3, 0.5, 0.7
    \item Compared predictions with and without NMS
    \item Generated visualizations for qualitative analysis
\end{enumerate}

\subsection{Inference Pipeline}

The inference process follows these steps:

\begin{enumerate}
    \item \textbf{Model Preparation}: Load trained model and set to evaluation mode
    \begin{lstlisting}
model.eval()
model.to(DEVICE)
    \end{lstlisting}
    
    \item \textbf{Image Processing}: Load and preprocess test image
    \begin{lstlisting}
image = Image.open(test_image_path).convert("RGB")
image_tensor = transforms.ToTensor()(image).unsqueeze(0).to(DEVICE)
    \end{lstlisting}
    
    \item \textbf{Forward Pass}: Generate predictions without gradient computation
    \begin{lstlisting}
with torch.no_grad():
    predictions = model(image_tensor)
    \end{lstlisting}
    
    \item \textbf{Post-processing}: Filter by confidence and apply NMS
\end{enumerate}

\subsection{Experimental Results}

\subsubsection{Raw Model Output}

The trained model produced \textbf{4 initial predictions} for the test image before any filtering. These represent all detected regions with their associated confidence scores and bounding boxes.

\subsubsection{Confidence Threshold Experiments}

Confidence thresholding dramatically affects the number of detections, filtering predictions based on model certainty. At threshold 0.3 (low), 3 detections are retained from the original 4 predictions (75\% retention), providing high recall but potentially including false positives. Both thresholds 0.5 and 0.7 (medium and high) retain exactly 2 detections (50\% retention), indicating that the second and third detections have confidence scores between 0.5 and 0.7, suggesting reasonable model confidence about these objects. At threshold 0.9 (very high), only 1 detection survives (25\% retention), representing the model's most confident prediction with maximum precision but potentially missing valid objects.

From this filtering pattern, we infer the approximate confidence score distribution: Detection 1 has confidence $\geq 0.9$ (survives all thresholds), Detections 2-3 fall between 0.5 and 0.9 (survive medium thresholds), and Detection 4 is below 0.3 (filtered at lowest threshold). This distribution indicates good model calibration with clear separation between high-confidence and low-confidence predictions.

\subsubsection{Non-Maximum Suppression (NMS) Analysis}

NMS eliminates redundant overlapping detections using Intersection over Union (IoU):

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Confidence} & \multicolumn{3}{c}{\textbf{Detections After NMS}} & \textbf{NMS} \\
\textbf{Threshold} & \textbf{IoU=0.3} & \textbf{IoU=0.5} & \textbf{IoU=0.7} & \textbf{Effect} \\
\midrule
0.3 & 3 & 3 & 3 & None \\
0.5 & 2 & 2 & 2 & None \\
0.7 & 2 & 2 & 2 & None \\
0.9 & 1 & 1 & 1 & None \\
\bottomrule
\end{tabular}
\caption{NMS impact at different IoU thresholds. No detections were suppressed.}
\label{tab:task4_nms}
\end{table}

\textbf{Key Observation:} NMS had \textbf{no effect} on detection count across all configurations, with both "No NMS" and "With NMS (IoU=0.5)" retaining exactly 2 detections at confidence threshold 0.5. This indicates that detected objects have minimal bounding box overlap (IoU < 0.3), demonstrating they are spatially distinct. The model produces tight, non-overlapping bounding boxes rather than multiple redundant predictions for the same object. This reflects both good localization quality and high-quality proposals from the Region Proposal Network. The test image's low object density with sufficient spacing between objects reduces the need for NMS. In denser scenes with more objects or lower-quality proposals, NMS would typically suppress 20-50\% of detections.

\subsection{Visual Analysis}

Three visualizations were generated to demonstrate detection quality. The \textbf{standard output} (\texttt{task4\_output\_conf0.5\_iou0.5.jpg}) shows 2 detections with confidence $\geq$ 0.5 after NMS (IoU=0.5). The \textbf{no NMS} version (\texttt{task4\_no\_nms.jpg}) shows the same 2 detections with confidence $\geq$ 0.5, confirming NMS had no effect. The \textbf{with NMS} version (\texttt{task4\_with\_nms.jpg}) is identical to the no NMS version, visually confirming that NMS suppressed nothing due to well-separated object locations.

\begin{figure}[H]
\centering
\begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\textwidth]{outputs/visualizations/task4_output_conf0.5_iou0.5.jpg}
    \caption{Standard detection (conf=0.5, IoU=0.5)}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\textwidth]{outputs/visualizations/task4_with_nms.jpg}
    \caption{With NMS applied}
\end{subfigure}
\caption{Task 4 inference results showing detected objects with bounding boxes. Both images are identical, confirming NMS had no suppression effect.}
\label{fig:task4_visual}
\end{figure}

\subsection{Threshold Selection Guidelines}

Based on experimental results:

\begin{table}[H]
\centering
\begin{tabular}{lp{8cm}}
\toprule
\textbf{Threshold} & \textbf{Use Case} \\
\midrule
0.3 & Maximum recall applications where missing objects is costly (e.g., safety inspection, surveillance). Expect some false positives. \\
0.5 & Balanced general-purpose detection. Filters obvious false positives while retaining reasonable recall. \textbf{Recommended default}. \\
0.7 & High-confidence applications where precision is important. May miss some valid but uncertain detections. \\
0.9 & Critical precision applications where false positives are unacceptable (e.g., autonomous driving decisions, medical diagnosis). Significant recall loss. \\
\bottomrule
\end{tabular}
\caption{Confidence threshold selection guidelines based on application requirements}
\label{tab:threshold_guidelines}
\end{table}

\subsection{NMS Configuration Guidelines}

Although NMS had no effect in this experiment, general guidelines recommend IoU=0.3 for aggressive suppression (removes boxes with >30\% overlap), suitable for well-separated objects or when redundancy must be minimized. The standard IoU=0.5 setting balances redundancy removal and detection retention, representing the recommended default. Conservative IoU=0.7 only suppresses heavily overlapping boxes, making it appropriate for densely packed objects or uncertain scenarios.

\subsection{Practical Deployment Considerations}

\textbf{Inference Speed and Deployment:} Forward pass on the A40 GPU requires less than 100ms per image, with confidence filtering taking under 1ms (negligible) and NMS computation under 5ms when few detections exist. Visualization adds 10-50ms depending on image size. For deployment, we recommend using confidence threshold 0.5 with NMS IoU 0.5 as the default configuration. Process multiple images in parallel batches to amortize model loading overhead, and adjust confidence dynamically based on real-time precision/recall monitoring. Use lower IoU thresholds for dense scenes and higher for sparse scenes, implementing a post-processing pipeline: confidence filter → NMS → size filtering (remove tiny/huge boxes).

\subsection{Key Findings}

\begin{enumerate}
    \item \textbf{Effective Confidence Filtering}: Model produces well-calibrated confidence scores with clear separation (one detection >0.9, two between 0.5-0.9, one <0.3).
    
    \item \textbf{NMS Not Always Necessary}: In this case, detections were already well-separated (IoU <0.3), demonstrating good model localization quality.
    
    \item \textbf{Threshold Sensitivity}: Changing confidence from 0.3 to 0.9 reduces detections from 3 to 1 (67\% reduction), highlighting the importance of threshold selection.
    
    \item \textbf{Stable NMS Behavior}: IoU threshold variation (0.3-0.7) had no impact, suggesting robust detection spacing.
    
    \item \textbf{Model Quality}: The clean detection pattern (no redundancy, clear confidence hierarchy) reflects the quality achieved through proper hyperparameter tuning and transfer learning from Tasks 1-2.
    
    \item \textbf{Practical Default}: Confidence=0.5 with NMS IoU=0.5 provides a robust starting point for deployment, filtering 50\% of raw predictions while maintaining high-confidence detections.
\end{enumerate}

\section{Overall Discussion}

\subsection{Best Practices Identified}

Through comprehensive experimentation, several best practices emerge:

\begin{enumerate}
    \item \textbf{Learning Rate Selection}: Use 0.0001 for fine-tuning pretrained Faster R-CNN. Higher rates (0.001) cause catastrophic failure; lower rates (0.00001) are safe but slower.
    
    \item \textbf{Training Duration}: Extended training (15 epochs) yields 16\% improvement over shorter training (10 epochs) without overfitting, justifying the computational cost.
    
    \item \textbf{Transfer Learning}: Full fine-tuning significantly outperforms frozen backbone (162\% better), demonstrating the need for dataset-specific feature adaptation.
    
    \item \textbf{Architecture Selection}: ResNet-50 provides the best accuracy-speed balance for this task. MobileNetV3 should only be chosen when inference speed is paramount.
    
    \item \textbf{Data Augmentation}: Surprisingly ineffective for this dataset. Use selectively based on deployment conditions rather than by default.
    
    \item \textbf{Batch Size}: Larger batches (4) offer significant speedup (33\%) with acceptable performance trade-off, making them attractive for rapid experimentation.
\end{enumerate}

\subsection{Computational Efficiency}

Training on the NVIDIA A40 enabled extensive exploration:

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{Configurations Tested} & \textbf{Total Time} \\
\midrule
Task 1: Hyperparameters & 6 & $\sim$2,293s ($\sim$38 min) \\
Task 2: Transfer Learning & 4 & $\sim$1,784s ($\sim$30 min) \\
Task 3: Augmentation & 4 & $\sim$1,553s ($\sim$26 min) \\
Task 4: Evaluation & Various & $\sim$94s ($\sim$2 min) \\
\midrule
\textbf{Total} & \textbf{14+ configurations} & \textbf{$\sim$2 hours} \\
\bottomrule
\end{tabular}
\caption{Computational resources utilized across all tasks}
\label{tab:compute_summary}
\end{table}

The A40's 46GB VRAM eliminated memory constraints, allowing experimentation with larger batch sizes and longer training runs that would be infeasible on consumer GPUs.

\subsection{Limitations and Future Work}

Several limitations should be addressed in future work:

\begin{enumerate}
    \item \textbf{Small Dataset}: With only 94 training images, conclusions about augmentation and overfitting are limited. Experiments on larger datasets (1000+ images) would provide more generalizable insights.
    
    \item \textbf{No Validation Set}: Training used the entire dataset without a held-out validation set, preventing proper hyperparameter tuning and overfitting detection. Future work should split data into train/val/test sets.
    
    \item \textbf{Limited Metrics}: Analysis focused on training loss without computing mAP, precision, recall, or IoU metrics. These are essential for comprehensive evaluation.
    
    \item \textbf{Single Domain}: All experiments used one dataset. Cross-domain evaluation (training on one dataset, testing on another) would assess generalization more rigorously.
    
    \item \textbf{Architecture Variants}: Only ResNet-50 and MobileNetV3 were tested. Modern architectures like EfficientNet, ResNeXt, or Vision Transformers might offer better performance.
    
    \item \textbf{Augmentation Debugging}: The horizontal flip failure requires deeper investigation. Systematic debugging with visualization and controlled experiments would identify the root cause.
    
    \item \textbf{Learning Rate Scheduling}: All experiments used constant learning rates. Schedulers (step decay, cosine annealing) could improve convergence.
    
    \item \textbf{Multi-GPU Training}: The A40 cluster has multiple GPUs. Distributed training could enable larger batch sizes and faster experimentation.
\end{enumerate}


\section{Conclusion}

This laboratory work systematically investigated Faster R-CNN for object detection on a small custom dataset. Learning rate selection proves critical, with 10x increases degrading performance by 178\% (0.252 vs 0.091 baseline). Extended training improves results by 16\% without overfitting. Transfer learning demonstrates that full fine-tuning outperforms frozen backbone by 139\%, confirming that pretrained ImageNet features require domain adaptation. Augmentation results surprise: ImageNet normalization improves performance 20\%, while horizontal flip degrades it 320\%. Architecture matters significantly, with ResNet-50 outperforming MobileNetV3 by 784\%.

The optimal configuration combines LR=0.0001, batch size 1, 15 epochs, full fine-tuning with ResNet-50, and ImageNet normalization, achieving final loss of 0.0601. This represents the culmination of systematic hyperparameter and architectural exploration enabled by A40 GPU computational power.

The NVIDIA A40 GPU's computational power enabled extensive exploration that would be prohibitive on consumer hardware, demonstrating the value of cluster computing for deep learning research. The systematic investigation across 14+ configurations provided insights that generalize beyond this specific dataset to inform practical object detection applications.

Future work should address the identified limitations, particularly the need for larger datasets, validation splits, comprehensive evaluation metrics, and debugging of the horizontal flip augmentation failure. Additionally, exploring modern architectures and advanced training techniques could push performance further.

\section*{Acknowledgments}

This work was conducted using the computing resources of the university cluster, specifically the NVIDIA A40 GPU node (Ampere architecture). The computational power provided enabled comprehensive experimentation that would not have been feasible on standard hardware.

\end{document}
